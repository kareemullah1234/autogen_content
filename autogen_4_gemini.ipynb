{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b834076-6332-4506-9585-417b7d00c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@AIzaSyC5WYmALyB74po0vj0wc7Ve7voV__ICyo4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1acb457-16f6-4a30-a030-d2c201a9a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='Paris\\n' usage=RequestUsage(prompt_tokens=7, completion_tokens=2) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=\"AIzaSyC5WYmALyB74po0vj0wc7Ve7voV__ICyo4\",\n",
    ")\n",
    "\n",
    "response = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(response)\n",
    "await model_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7884b583-f819-4ae0-ae4d-065e356ac322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response type: <class 'autogen_core.models._types.CreateResult'>\n",
      "Response repr (truncated to 2000 chars):\n",
      "CreateResult(finish_reason='stop', content='Paris\\n', usage=RequestUsage(prompt_tokens=7, completion_tokens=2), cached=False, logprobs=None, thought=None)\n",
      "\n",
      "Extracted reply:\n",
      "Paris\n",
      "\n",
      "\n",
      "Client model_info attribute (if present):\n",
      "{'vision': True, 'function_calling': True, 'json_output': True, 'family': 'gemini-1.5-flash', 'structured_output': True, 'multiple_system_messages': False}\n",
      "\n",
      "Agent public methods: ['close', 'component_config_schema', 'component_description', 'component_label', 'component_provider_override', 'component_type', 'component_version', 'description', 'dump_component', 'load_component', 'load_state', 'model_context', 'name', 'on_messages', 'on_messages_stream', 'on_pause', 'on_reset', 'on_resume', 'produced_message_types', 'required_class_vars', 'run', 'run_stream', 'save_state']\n",
      "agent.run signature: (*, task: Union[str, autogen_agentchat.messages.BaseChatMessage, Sequence[autogen_agentchat.messages.BaseChatMessage], NoneType] = None, cancellation_token: autogen_core._cancellation_token.CancellationToken | None = None, output_task_messages: bool = True) -> autogen_agentchat.base._task.TaskResult\n",
      "\n",
      "ask_agent_text result (repr): 'Paris\\n'\n",
      "ask_agent_text extracted text: 'Paris\\n'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff04c4d-d4b5-4f1f-a50e-899a6a2388bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter one-cell: inspect agent API and robustly call it (top-level await)\n",
    "import os, inspect, traceback\n",
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "# Read API key from environment (recommended). If you previously put it in a variable, set env instead:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # avoid putting secrets in cells\n",
    "# openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# if not openai_key:\n",
    "#     print(\"Warning: OPENAI_API_KEY not set in env. Set it and re-run. (Do not paste the key in chat.)\")\n",
    "\n",
    "# # Create or reuse the model client\n",
    "# model_client = OpenAIChatCompletionClient(\n",
    "#     model=\"gemini-1.5-flash-8b\",\n",
    "#     api_key=openai_key or \"\"  # empty string if you rely on env in the client\n",
    "# )\n",
    "\n",
    "# If the Agent requires model_info[\"function_calling\"], ensure it's present (OpenAI models support it).\n",
    "if not getattr(model_client, \"model_info\", None):\n",
    "    model_client.model_info = {}\n",
    "model_client.model_info.setdefault(\"function_calling\", True)\n",
    "\n",
    "# Example async tool\n",
    "async def web_search(query: str) -> str:\n",
    "    # replace with a real web call in production\n",
    "    return \"AutoGen is a programming framework for building multi-agent applications.\"\n",
    "\n",
    "# Create the AssistantAgent\n",
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    tools=[web_search],\n",
    "    system_message=\"You are a helpful assistant. Use tools when helpful.\"\n",
    ")\n",
    "\n",
    "# Helper: friendly extractor for common response shapes\n",
    "def extract_text_from_response(resp):\n",
    "    try:\n",
    "        # list-like response\n",
    "        return resp[0].content\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        # object with .content\n",
    "        return getattr(resp, \"content\", None) or getattr(resp, \"text\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # dict-style (OpenAI-like)\n",
    "    if isinstance(resp, dict):\n",
    "        try:\n",
    "            choices = resp.get(\"choices\")\n",
    "            if choices and isinstance(choices, list):\n",
    "                message = choices[0].get(\"message\") or {}\n",
    "                return message.get(\"content\") or choices[0].get(\"text\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return repr(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebef954-87c9-479a-84a1-2e820e091f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent methods: ['close', 'component_config_schema', 'component_description', 'component_label', 'component_provider_override', 'component_type', 'component_version', 'description', 'dump_component', 'load_component', 'load_state', 'model_context', 'name', 'on_messages', 'on_messages_stream', 'on_pause', 'on_reset', 'on_resume', 'produced_message_types', 'required_class_vars', 'run', 'run_stream', 'save_state']\n",
      "agent.run signature: (*, task: Union[str, autogen_agentchat.messages.BaseChatMessage, Sequence[autogen_agentchat.messages.BaseChatMessage], NoneType] = None, cancellation_token: autogen_core._cancellation_token.CancellationToken | None = None, output_task_messages: bool = True) -> autogen_agentchat.base._task.TaskResult\n",
      "Look for helper methods such as: create, run, run_async, run_sync, act, step, receive\n",
      "Fallback: calling model_client.create(...)\n",
      "Final reply: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_base_client.py\", line 1526, in request\n",
      "    response = await self._client.send(\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py\", line 1616, in send\n",
      "    raise RuntimeError(\"Cannot send a request, as the client has been closed.\")\n",
      "RuntimeError: Cannot send a request, as the client has been closed.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_14236\\1784873744.py\", line 110, in ask_agent\n",
      "    resp = await model_client.create([UserMessage(content=user_text, source=\"user\")])\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 691, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2544, in create\n",
      "    return await self._post(\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_base_client.py\", line 1558, in request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n"
     ]
    }
   ],
   "source": [
    "# Inspect available agent methods and signature for guidance\n",
    "print(\"Agent methods:\", [m for m in dir(agent) if not m.startswith(\"_\")])\n",
    "try:\n",
    "    print(\"agent.run signature:\", inspect.signature(agent.run))\n",
    "except Exception:\n",
    "    print(\"Could not get signature for agent.run\")\n",
    "print(\"Look for helper methods such as: create, run, run_async, run_sync, act, step, receive\")\n",
    "\n",
    "# Robust caller that tries common shapes and falls back to model_client.create\n",
    "async def ask_agent(user_text: str):\n",
    "    # 1) Try agent.run(user_text)\n",
    "    try:\n",
    "        maybe = agent.run(user_text)\n",
    "        if inspect.isawaitable(maybe):\n",
    "            maybe = await maybe\n",
    "        print(\"Used: agent.run(user_text)\")\n",
    "        return maybe\n",
    "    except TypeError:\n",
    "        pass\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 2) Try agent.run(UserMessage(...))\n",
    "    try:\n",
    "        maybe = agent.run(UserMessage(content=user_text, source=\"user\"))\n",
    "        if inspect.isawaitable(maybe):\n",
    "            maybe = await maybe\n",
    "        print(\"Used: agent.run(UserMessage(...))\")\n",
    "        return maybe\n",
    "    except TypeError:\n",
    "        pass\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 3) Try other common method names\n",
    "    for name in (\"create\", \"run_async\", \"run_sync\", \"act\", \"step\", \"receive\"):\n",
    "        if hasattr(agent, name):\n",
    "            try:\n",
    "                fn = getattr(agent, name)\n",
    "                maybe = fn(user_text)\n",
    "                if inspect.isawaitable(maybe):\n",
    "                    maybe = await maybe\n",
    "                print(f\"Used: agent.{name}(user_text)\")\n",
    "                return maybe\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "\n",
    "    # 4) Fallback: call model_client.create directly\n",
    "    try:\n",
    "        print(\"Fallback: calling model_client.create(...)\")\n",
    "        resp = await model_client.create([UserMessage(content=user_text, source=\"user\")])\n",
    "        return extract_text_from_response(resp)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Example call\n",
    "reply = await ask_agent(\"What is the capital of France?\")\n",
    "print(\"Final reply:\", reply)\n",
    "\n",
    "# Close client when done\n",
    "await model_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4a5d262-2238-4368-a44a-1117c2b4e752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87560af-6dfc-460e-938e-da882aacf3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for .env at: C:\\Users\\LENOVO\\autogen\\.env\n",
      ".env loaded: True\n",
      "GOOGLE_API_KEY present in env? True\n"
     ]
    }
   ],
   "source": [
    "# Run in a notebook cell\n",
    "#%pip install -q python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# If your .env is in the same folder as the notebook, this will find it automatically.\n",
    "# Otherwise, give the full path: load_dotenv(\"C:/Users/LENOVO/autogen/.env\")\n",
    "env_path = Path.cwd() / \".env\"\n",
    "print(\"Looking for .env at:\", env_path)\n",
    "loaded = load_dotenv(dotenv_path=str(env_path))\n",
    "print(\".env loaded:\", loaded)\n",
    "print(\"GOOGLE_API_KEY present in env?\", bool(os.environ.get(\"GOOGLE_API_KEY\")))\n",
    "\n",
    "# Now you can call your make_gemini_client safely; it will read os.environ[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f836021-f619-4bea-a06b-9ba3c63a9024",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_gemini_client.<locals>.create() got an unexpected keyword argument 'tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 118\u001b[0m\n\u001b[0;32m    109\u001b[0m agent_loop \u001b[38;5;241m=\u001b[39m AssistantAgent(\n\u001b[0;32m    110\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    111\u001b[0m     model_client\u001b[38;5;241m=\u001b[39mgemini_client_loop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m     max_tool_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    115\u001b[0m )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# --- Run a quick test (top-level await in notebook) ---\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent_no_parallel_tool_call\u001b[38;5;241m.\u001b[39mrun(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFind the capital of France and summarize in one line.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Extract the final assistant message robustly\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\autogen_agentchat\\agents\\_base_chat_agent.py:149\u001b[0m, in \u001b[0;36mBaseChatAgent.run\u001b[1;34m(self, task, cancellation_token, output_task_messages)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 149\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_messages(input_messages, cancellation_token)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39minner_messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     output_messages \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39minner_messages\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:896\u001b[0m, in \u001b[0;36mAssistantAgent.on_messages\u001b[1;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_messages\u001b[39m(\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    884\u001b[0m     messages: Sequence[BaseChatMessage],\n\u001b[0;32m    885\u001b[0m     cancellation_token: CancellationToken,\n\u001b[0;32m    886\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m    887\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Process incoming messages and generate a response.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \n\u001b[0;32m    889\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m        Response containing the agent's reply\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 896\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_messages_stream(messages, cancellation_token):\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[0;32m    898\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:953\u001b[0m, in \u001b[0;36mAssistantAgent.on_messages_stream\u001b[1;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# STEP 4: Run the first inference\u001b[39;00m\n\u001b[0;32m    952\u001b[0m model_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 953\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m inference_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_llm(\n\u001b[0;32m    954\u001b[0m     model_client\u001b[38;5;241m=\u001b[39mmodel_client,\n\u001b[0;32m    955\u001b[0m     model_client_stream\u001b[38;5;241m=\u001b[39mmodel_client_stream,\n\u001b[0;32m    956\u001b[0m     system_messages\u001b[38;5;241m=\u001b[39msystem_messages,\n\u001b[0;32m    957\u001b[0m     model_context\u001b[38;5;241m=\u001b[39mmodel_context,\n\u001b[0;32m    958\u001b[0m     workbench\u001b[38;5;241m=\u001b[39mworkbench,\n\u001b[0;32m    959\u001b[0m     handoff_tools\u001b[38;5;241m=\u001b[39mhandoff_tools,\n\u001b[0;32m    960\u001b[0m     agent_name\u001b[38;5;241m=\u001b[39magent_name,\n\u001b[0;32m    961\u001b[0m     cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token,\n\u001b[0;32m    962\u001b[0m     output_content_type\u001b[38;5;241m=\u001b[39moutput_content_type,\n\u001b[0;32m    963\u001b[0m     message_id\u001b[38;5;241m=\u001b[39mmessage_id,\n\u001b[0;32m    964\u001b[0m ):\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inference_output, CreateResult):\n\u001b[0;32m    966\u001b[0m         model_result \u001b[38;5;241m=\u001b[39m inference_output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:1107\u001b[0m, in \u001b[0;36mAssistantAgent._call_llm\u001b[1;34m(cls, model_client, model_client_stream, system_messages, model_context, workbench, handoff_tools, agent_name, cancellation_token, output_content_type, message_id)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1107\u001b[0m     model_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mmodel_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcancellation_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_content_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n",
      "\u001b[1;31mTypeError\u001b[0m: make_gemini_client.<locals>.create() got an unexpected keyword argument 'tools'"
     ]
    }
   ],
   "source": [
    "# One Jupyter cell: create functional Gemini clients, two AssistantAgent instances, run a test, close clients.\n",
    "import os, json, types, httpx\n",
    "from typing import List\n",
    "from autogen_core.models import UserMessage\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "# --- lightweight functional Gemini client factory (reads GOOGLE_API_KEY from env) ---\n",
    "def make_gemini_client(\n",
    "    model: str,\n",
    "    api_key: str | None = None,\n",
    "    temperature: float = 0.0,\n",
    "    max_output_tokens: int = 512,\n",
    "    function_calling: bool = False,\n",
    "    streaming: bool = False,\n",
    "    vision: bool = False,\n",
    "    parallel_tool_calls: bool = True,\n",
    "):\n",
    "    api_key = api_key or os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"Set GOOGLE_API_KEY in your environment before running this cell.\")\n",
    "\n",
    "    async_client = httpx.AsyncClient(timeout=60.0)\n",
    "\n",
    "    async def create(messages: List[UserMessage]):\n",
    "        # Flatten messages into a single prompt\n",
    "        parts = []\n",
    "        for m in messages:\n",
    "            role = getattr(m, \"source\", None) or getattr(m, \"role\", None) or \"user\"\n",
    "            parts.append(f\"{str(role).upper()}: {getattr(m, 'content', '')}\")\n",
    "        prompt_text = \"\\n\".join(parts).strip()\n",
    "\n",
    "        url = f\"https://generativelanguage.googleapis.com/v1/{model}:generate?key={api_key}\"\n",
    "        body = {\n",
    "            \"prompt\": {\"text\": prompt_text},\n",
    "            \"temperature\": temperature,\n",
    "            \"maxOutputTokens\": max_output_tokens,\n",
    "        }\n",
    "\n",
    "        r = await async_client.post(url, json=body)\n",
    "        r.raise_for_status()\n",
    "        j = r.json()\n",
    "\n",
    "        # Extract likely text fields\n",
    "        text = None\n",
    "        if isinstance(j, dict):\n",
    "            if \"candidates\" in j and isinstance(j[\"candidates\"], list) and j[\"candidates\"]:\n",
    "                cand = j[\"candidates\"][0]\n",
    "                if isinstance(cand, dict):\n",
    "                    text = cand.get(\"output\") or cand.get(\"content\") or cand.get(\"text\") or cand.get(\"outputText\")\n",
    "            if text is None:\n",
    "                text = j.get(\"output\") or j.get(\"outputText\") or j.get(\"reply\") or j.get(\"text\")\n",
    "            if text is None:\n",
    "                try:\n",
    "                    if \"candidates\" in j and j[\"candidates\"]:\n",
    "                        c0 = j[\"candidates\"][0]\n",
    "                        if isinstance(c0, dict) and \"content\" in c0 and isinstance(c0[\"content\"], dict):\n",
    "                            text = c0[\"content\"].get(\"text\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if text is None:\n",
    "            text = json.dumps(j)\n",
    "\n",
    "        # Return same shape autogen expects: list-like with .content\n",
    "        return [types.SimpleNamespace(content=text)]\n",
    "\n",
    "    async def close():\n",
    "        await async_client.aclose()\n",
    "\n",
    "    ns = types.SimpleNamespace(\n",
    "        create=create,\n",
    "        close=close,\n",
    "        model_info={\"vision\": vision, \"function_calling\": function_calling, \"streaming\": streaming},\n",
    "    )\n",
    "    ns.parallel_tool_calls = parallel_tool_calls\n",
    "    return ns\n",
    "\n",
    "# --- Example async tool the agent can (attempt to) call ---\n",
    "async def web_search(q: str) -> str:\n",
    "    # simulated search (replace with real HTTP call if needed)\n",
    "    return f\"(web_search simulated) results for: {q}\"\n",
    "\n",
    "# --- Configure your Gemini model name (change if needed) ---\n",
    "gemini_model = \"models/gemini-1.5-flash-8b\"  # adjust if you have a different resource\n",
    "\n",
    "# --- Create clients and agents (functional approach) ---\n",
    "gemini_client_no_parallel = make_gemini_client(\n",
    "    model=gemini_model,\n",
    "    function_calling=True,      # True if you want Agent to attempt function-style calls\n",
    "    streaming=False,\n",
    "    vision=False,\n",
    "    parallel_tool_calls=False,  # mirror your OpenAI example\n",
    ")\n",
    "\n",
    "gemini_client_loop = make_gemini_client(\n",
    "    model=gemini_model,\n",
    "    function_calling=True,\n",
    "    streaming=False,\n",
    "    vision=False,\n",
    "    parallel_tool_calls=False,\n",
    ")\n",
    "\n",
    "agent_no_parallel_tool_call = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=gemini_client_no_parallel,\n",
    "    tools=[web_search],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    ")\n",
    "\n",
    "agent_loop = AssistantAgent(\n",
    "    name=\"assistant_loop\",\n",
    "    model_client=gemini_client_loop,\n",
    "    tools=[web_search],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    "    max_tool_iterations=10,\n",
    ")\n",
    "\n",
    "# --- Run a quick test (top-level await in notebook) ---\n",
    "result = await agent_no_parallel_tool_call.run(task=\"Find the capital of France and summarize in one line.\")\n",
    "# Extract the final assistant message robustly\n",
    "try:\n",
    "    final_msg = result.messages[-1]\n",
    "    content = getattr(final_msg, \"content\", getattr(final_msg, \"text\", repr(final_msg)))\n",
    "except Exception:\n",
    "    content = repr(result)\n",
    "print(\"Agent reply:\", content)\n",
    "\n",
    "# --- Close clients ---\n",
    "await gemini_client_no_parallel.close()\n",
    "await gemini_client_loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a3921-22aa-464e-aaae-8f8b3600953d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
